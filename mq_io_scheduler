blk_mq_make_request

首先看看能不能在plug队列里合并，如果可以直接返回，否则找一个空闲的request（不是分配，是提前分配好的）
如果是flush标志，直接插入到hw 队列里，跳过软件队列，否则
如果有plug但是硬件队列是有一个，这个时候老老实实插入到plug队列里面，（因为只有一个队列，不能过分issue）
    如果plug数目比较多了，把plug队列里的请求冲刷到sw队列
如果有plug而且很多hw队列(有多个hw队列，就可以尝试更快地issue)，但是不排斥plug merge动作（如果排斥，plug队列就不用这么处理），除了插入到plug里面，
    同一队列的请求应当被直接issue给驱动处理，这个动作保证了plug对队列里同一个request_queue的请求只有一个
如果是同步请求并且hw队列大于1(很多，所以同步操作可以任性一些），试试直接issue，否则即使是同步操作，只有一个hw队列，也只能落到调度器或者sw手里
先看是不是有调度算法，有的话交给调度算法，否则直接放到软件队列


快速动作有两个：一个是直接插入到hw队列，一个更快的是issue给驱动
正常情况是想插入到sw队列，当然有plug要先考虑plug机制，没有plug要先考虑调度算法，毕竟unplug后才会放到调度算法的势力范围



7 /*  
  8  * Tag address space map.
  9  */ 
 10 struct blk_mq_tags {
 11     unsigned int nr_tags;
 12     unsigned int nr_reserved_tags;
 13 
 14     atomic_t active_queues;
 15 
 16     struct sbitmap_queue bitmap_tags;
 17     struct sbitmap_queue breserved_tags;
 18 
 19     struct request **rqs; 
 20     struct request **static_rqs; // 提前分配好的request，以tag号为索引
 21     struct list_head page_list;
 22 };       





 
161 struct blk_mq_alloc_data {
162     /* input parameter */
163     struct request_queue *q;
164     blk_mq_req_flags_t flags;
165     unsigned int shallow_depth;
166 
167     /* input & output parameter */
168     struct blk_mq_ctx *ctx;
169     struct blk_mq_hw_ctx *hctx;
170 };   
171 
 
 
 
  137 /*
 138  * Try to put the fields that are referenced together in the same cacheline.
 139  *
 140  * If you modify this structure, make sure to update blk_rq_init() and
 141  * especially blk_mq_rq_ctx_init() to take care of the added fields.
 142  */
 143 struct request {
 144     struct request_queue *q;  //属于哪一个请求队列
 145     struct blk_mq_ctx *mq_ctx;  //属于哪个sw 队列
 146 
 147     int cpu;
 148     unsigned int cmd_flags;     /* op and common flags */
 149     req_flags_t rq_flags;
 150 
 151     int internal_tag;  //有调度算法的tag号，如果没有调度算法，则为-1
 152 
 153     /* the following two fields are internal, NEVER access directly */
 154     unsigned int __data_len;    /* total data len */
 155     int tag; // 没有调度算法的tag号，如果有调度算法，则为-1
 156     sector_t __sector;      /* sector cursor */
 157 
 158     struct bio *bio;
 159     struct bio *biotail;
 160 
 161     struct list_head queuelist;
 162 
 163     /*
 164      * The hash is used inside the scheduler, and killed once the
 165      * request reaches the dispatch list. The ipi_list is only used
 166      * to queue the request for softirq completion, which is long
 167      * after the request has been unhashed (and even removed from
 168      * the dispatch list).
 169      */
 170     union {
 171         struct hlist_node hash; /* merge hash */
 172         struct list_head ipi_list;
 173     };
 174 
 175     /*
 176      * The rb_node is only used inside the io scheduler, requests
 177      * are pruned when moved to the dispatch queue. So let the
 178      * completion_data share space with the rb_node.
 179      */
 180     union {
 181         struct rb_node rb_node; /* sort/lookup */
 182         struct bio_vec special_vec;
 183         void *completion_data;
 184         int error_count; /* for legacy drivers, don't use */
 185     };
 186 
 187     /*
 188      * Three pointers are available for the IO schedulers, if they need
 189      * more they have to dynamically allocate it.  Flush requests are
 190      * never put on the IO scheduler. So let the flush fields share
 191      * space with the elevator data.
 192      */
 193     union {
 194         struct {
 195             struct io_cq        *icq;
 196             void            *priv[2];
 197         } elv;
 198 
 199         struct {
 200             unsigned int        seq;
 201             struct list_head    list;
 202             rq_end_io_fn        *saved_end_io;
 203         } flush;
 204     };
 205 
 206     struct gendisk *rq_disk;
 207     struct hd_struct *part;
 208     unsigned long start_time;
 209     struct blk_issue_stat issue_stat;
 210     /* Number of scatter-gather DMA addr+len pairs after
 211      * physical address coalescing is performed.
 212      */
 213     unsigned short nr_phys_segments;
 214 
 215 #if defined(CONFIG_BLK_DEV_INTEGRITY)
 216     unsigned short nr_integrity_segments;
 217 #endif
 218 
 219     unsigned short write_hint;
 220     unsigned short ioprio;
 221 
 222     unsigned int timeout;
 223 
 224     void *special;      /* opaque pointer available for LLD use */
 225 
 226     unsigned int extra_len; /* length of alignment and padding */
 227 
 228     /*
 229      * On blk-mq, the lower bits of ->gstate (generation number and
 230      * state) carry the MQ_RQ_* state value and the upper bits the
 231      * generation number which is monotonically incremented and used to
 232      * distinguish the reuse instances.
 233      *
 234      * ->gstate_seq allows updates to ->gstate and other fields
 235      * (currently ->deadline) during request start to be read
236      * atomically from the timeout path, so that it can operate on a
 237      * coherent set of information.
 238      */
 239     seqcount_t gstate_seq;
 240     u64 gstate;
 241 
 242     /*
 243      * ->aborted_gstate is used by the timeout to claim a specific
 244      * recycle instance of this request.  See blk_mq_timeout_work().
 245      */
 246     struct u64_stats_sync aborted_gstate_sync;
 247     u64 aborted_gstate;
 248 
 249     /* access through blk_rq_set_deadline, blk_rq_deadline */
 250     unsigned long __deadline;
 251 
 252     struct list_head timeout_list;
 253 
 254     union {
 255         struct __call_single_data csd;
 256         u64 fifo_time;
 257     };
 258 
 259     /*
 260      * completion callback.
 261      */
 262     rq_end_io_fn *end_io;  //回调函数
 263     void *end_io_data;
 264 
 265     /* for bidi */
 266     struct request *next_rq;
 267 
 268 #ifdef CONFIG_BLK_CGROUP
 269     struct request_list *rl;        /* rl this rq is alloced from */         // 属于那个request 链表
 270     unsigned long long start_time_ns;
 271     unsigned long long io_start_time_ns;    /* when passed to hardware */
 272 #endif
 273 };




 
 
 436 struct request_queue {
 437     /*
 438      * Together with queue_head for cacheline sharing
 439      */
 440     struct list_head    queue_head;
 441     struct request      *last_merge;
 442     struct elevator_queue   *elevator;  // 是否有调度算法
 443     int         nr_rqs[2];  /* # allocated [a]sync rqs */
 444     int         nr_rqs_elvpriv; /* # allocated rqs w/ elvpriv */
 445 
 446     atomic_t        shared_hctx_restart;
 447 
 448     struct blk_queue_stats  *stats;
 449     struct rq_wb        *rq_wb;
 450 
 451     /*
 452      * If blkcg is not used, @q->root_rl serves all requests.  If blkcg
 453      * is used, root blkg allocates from @q->root_rl and all other
 454      * blkgs from their own blkg->rl.  Which one to use should be
 455      * determined using bio_request_list().
 456      */
 457     struct request_list root_rl;
 458 
 459     request_fn_proc     *request_fn;
 460     make_request_fn     *make_request_fn;
 461     poll_q_fn       *poll_fn;
 462     prep_rq_fn      *prep_rq_fn;
 463     unprep_rq_fn        *unprep_rq_fn;
 464     softirq_done_fn     *softirq_done_fn;
 465     rq_timed_out_fn     *rq_timed_out_fn;
 466     dma_drain_needed_fn *dma_drain_needed;
 467     lld_busy_fn     *lld_busy_fn;
 468     /* Called just after a request is allocated */
 469     init_rq_fn      *init_rq_fn;
 470     /* Called just before a request is freed */
 471     exit_rq_fn      *exit_rq_fn;
 472     /* Called from inside blk_get_request() */
 473     void (*initialize_rq_fn)(struct request *rq);
 474 
 475     const struct blk_mq_ops *mq_ops;   // mq ops
 476 
 477     unsigned int        *mq_map;  // 影射表
 478 
 479     /* sw queues */
 480     struct blk_\ __percpu  *queue_ctx;   // ctx队列,根据cpu取
 481     unsigned int        nr_queues;
 482 
 483     unsigned int        queue_depth;
 484 
 485     /* hw dispatch queues */
 486     struct blk_mq_hw_ctx    **queue_hw_ctx;  // 被影射的硬件hctx
 487     unsigned int        nr_hw_queues;
 488 
 489     /*
 490      * Dispatch queue sorting
 491      */
 492     sector_t        end_sector;
 493     struct request      *boundary_rq;
 494 
 495     /*
 496      * Delayed queue handling
 497      */
 498     struct delayed_work delay_work;
 499 
 500     struct backing_dev_info *backing_dev_info;
 501 
 502     /*
 503      * The queue owner gets to use this for whatever they like.
 504      * ll_rw_blk doesn't touch it.
 505      */
 506     void            *queuedata;
 507 
 508     /*
 509      * various queue flags, see QUEUE_* below
 510      */
 511     unsigned long       queue_flags;
 512 
 513     /*
 514      * ida allocated id for this queue.  Used to index queues from
 515      * ioctx.
 516      */
 517     int         id;
 518 
 519     /*
 520      * queue needs bounce pages for pages above this limit
 521      */
 522     gfp_t           bounce_gfp;
 523 
 524     /*
 525      * protects queue structures from reentrancy. ->__queue_lock should
 526      * _never_ be used directly, it is queue private. always use
 527      * ->queue_lock.
 528      */
 529     spinlock_t      __queue_lock;
 530     spinlock_t      *queue_lock;
 531 
 532     /*
 533      * queue kobject
 534      */
 535     struct kobject kobj;
 536 
 537     /*
 538      * mq queue kobject
 539      */
 540     struct kobject mq_kobj;
 541 
 542 #ifdef  CONFIG_BLK_DEV_INTEGRITY
 543     struct blk_integrity integrity;
 544 #endif  /* CONFIG_BLK_DEV_INTEGRITY */
 545 
 546 #ifdef CONFIG_PM
 547     struct device       *dev;
 548     int         rpm_status;
 549     unsigned int        nr_pending;
 550 #endif
 551 
 552     /*
 553      * queue settings
 554      */
 555     unsigned long       nr_requests;    /* Max # of requests */
 556     unsigned int        nr_congestion_on;
 557     unsigned int        nr_congestion_off;
 558     unsigned int        nr_batching;
 559 
 560     unsigned int        dma_drain_size;
 561     void            *dma_drain_buffer;
 562     unsigned int        dma_pad_mask;
 563     unsigned int        dma_alignment;
 564 
 565     struct blk_queue_tag    *queue_tags;
 566     struct list_head    tag_busy_list;
 567 
 568     unsigned int        nr_sorted;
 569     unsigned int        in_flight[2];
 570 
 571     /*
 572      * Number of active block driver functions for which blk_drain_queue()
 573      * must wait. Must be incremented around functions that unlock the
 574      * queue_lock internally, e.g. scsi_request_fn().
 575      */
 576     unsigned int        request_fn_active;
 577 
 578     unsigned int        rq_timeout;
 579     int         poll_nsec;
 580 
 581     struct blk_stat_callback    *poll_cb;
 582     struct blk_rq_stat  poll_stat[BLK_MQ_POLL_STATS_BKTS];
 583 
 584     struct timer_list   timeout;
 585     struct work_struct  timeout_work;
 586     struct list_head    timeout_list;
 587 
 588     struct list_head    icq_list;
 589 #ifdef CONFIG_BLK_CGROUP
 590     DECLARE_BITMAP      (blkcg_pols, BLKCG_MAX_POLS);
 591     struct blkcg_gq     *root_blkg;
 592     struct list_head    blkg_list;
 593 #endif
  594 
 595     struct queue_limits limits;
 596 
 597     /*
 598      * Zoned block device information for request dispatch control.
 599      * nr_zones is the total number of zones of the device. This is always
 600      * 0 for regular block devices. seq_zones_bitmap is a bitmap of nr_zones
 601      * bits which indicates if a zone is conventional (bit clear) or
 602      * sequential (bit set). seq_zones_wlock is a bitmap of nr_zones
 603      * bits which indicates if a zone is write locked, that is, if a write
 604      * request targeting the zone was dispatched. All three fields are
 605      * initialized by the low level device driver (e.g. scsi/sd.c).
 606      * Stacking drivers (device mappers) may or may not initialize
 607      * these fields.
 608      *
 609      * Reads of this information must be protected with blk_queue_enter() /
 610      * blk_queue_exit(). Modifying this information is only allowed while
 611      * no requests are being processed. See also blk_mq_freeze_queue() and
 612      * blk_mq_unfreeze_queue().
 613      */
 614     unsigned int        nr_zones;
 615     unsigned long       *seq_zones_bitmap;
 616     unsigned long       *seq_zones_wlock;
 617 
 618     /*
 619      * sg stuff
 620      */
 621     unsigned int        sg_timeout;
 622     unsigned int        sg_reserved_size;
 623     int         node;
 624 #ifdef CONFIG_BLK_DEV_IO_TRACE
 625     struct blk_trace    *blk_trace;
 626     struct mutex        blk_trace_mutex;
 627 #endif
 628     /*
 629      * for flush operations
 630      */
 631     struct blk_flush_queue  *fq;  // flush 队列
 632 
 633     struct list_head    requeue_list;
 634     spinlock_t      requeue_lock;
 635     struct delayed_work requeue_work;
 636 
 637     struct mutex        sysfs_lock;
 638 
 639     int         bypass_depth;
 640     atomic_t        mq_freeze_depth;
 641 
 642 #if defined(CONFIG_BLK_DEV_BSG)
 643     bsg_job_fn      *bsg_job_fn;
 644     struct bsg_class_device bsg_dev;
 645 #endif
 646 
 647 #ifdef CONFIG_BLK_DEV_THROTTLING
 648     /* Throttle data */
 649     struct throtl_data *td;
 650 #endif
 651     struct rcu_head     rcu_head;
 652     wait_queue_head_t   mq_freeze_wq;
 653     struct percpu_ref   q_usage_counter;
 654     struct list_head    all_q_node;
 655 
 656     struct blk_mq_tag_set   *tag_set;
 657     struct list_head    tag_set_list;
 658     struct bio_set      *bio_split;
 659 
 660 #ifdef CONFIG_BLK_DEBUG_FS
 661     struct dentry       *debugfs_dir;
 662     struct dentry       *sched_debugfs_dir;
 663 #endif
 664 
 665     bool            mq_sysfs_init_done;
 666 
 667     size_t          cmd_size;
 668     void            *rq_alloc_data;
 669 
 670     struct work_struct  release_work;
 671 
 672 #define BLK_MAX_WRITE_HINTS 5
 673     u64         write_hints[BLK_MAX_WRITE_HINTS];
 674 };


